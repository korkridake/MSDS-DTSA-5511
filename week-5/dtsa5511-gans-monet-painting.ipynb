{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DTSA5511 - GANs Monet Painting \n\n**Author** - Korkrid Akepanidtaworn, University of Colorado Boulder, Masters in Data Science\n**Date** - August, 20,2024\n\n## Project Goal\n\n- Computer vision has advanced tremendously in recent years and GANs are now capable of mimicking objects in a very convincing way. But creating museum-worthy masterpieces is thought of to be, well, more art than science. So can (data) science, in the form of GANs, trick classifiers into believing you’ve created a true Monet? That’s the challenge you’ll take on!\n- My goal is to build a GAN that generates 7,000 to 10,000 Monet-style images. \n- A GAN consists of at least two neural networks: a generator model and a discriminator model. The generator is a neural network that creates the images. For our competition, you should generate images in the style of Monet. This generator is trained using a discriminator.\n- The two models will work against each other, with the generator trying to trick the discriminator, and the discriminator trying to accurately classify the real vs. generated images.\n- [I’m Something of a Painter Myself | Kaggle](https://www.kaggle.com/competitions/gan-getting-started)\n\n## Dataset Description\n\n**Dataset Structure**\n\nThe dataset is organized into four directories:\n* `monet_tfrec`: Contains Monet paintings in TFRecord format (256x256).\n* `monet_jpg`: Contains Monet paintings in JPEG format (256x256).\n* `photo_tfrec`: Contains photos in TFRecord format (256x256).\n* `photo_jpg`: Contains photos in JPEG format (256x256).\n\n**Data Equivalence**\n\n* `monet_tfrec` and `monet_jpg` directories contain identical Monet painting images.\n* `photo_tfrec` and `photo_jpg` directories contain identical photo images.\n\n**Recommended Data Format**\n\nWhile JPEG images are provided, we strongly recommend using TFRecords as a starting point. Working with TFRecords can be a valuable learning experience for a new data format.\n\n**Dataset Purpose**\n\n* **Monet directories:** Utilize the Monet paintings for training your model.\n* **Photo directories:** Apply a Monet-style transformation to these photos and submit the generated JPEG images in a zip file. A maximum of 10,000 images is allowed.\n\n**Submission Guidelines**\n\n* The submitted images can be entirely generated Monet-style art without relying solely on transformed photos.\n* Explore other GAN architectures like DCGAN for creating Monet-style art from scratch.\n* Consider using the CycleGAN dataset to experiment with different artistic styles.\n\n**File Details**\n\n* `monet_jpg`: 300 Monet paintings (256x256 JPEG)\n* `monet_tfrec`: 300 Monet paintings (256x256 TFRecord)\n* `photo_jpg`: 7028 photos (256x256 JPEG)\n* `photo_tfrec`: 7028 photos (256x256 TFRecord)","metadata":{}},{"cell_type":"code","source":"# Check Python version\n!python --version","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\n# Standard libraries\nimport random  # For generating random numbers and shuffling\nimport shutil  # For file operations like copying and moving\n\n# Numerical and data manipulation libraries\nimport numpy as np  # For numerical operations and arrays\nimport pandas as pd  # For data manipulation and analysis\n\n# Image handling and visualization libraries\nimport matplotlib.image as mpimg  # For reading image files\nimport matplotlib.pyplot as plt  # For creating plots and visualizations\nfrom PIL import Image  # For opening and manipulating images\n\n# OS and file path utilities\nimport os\nfrom os import listdir  # For listing files in a directory\nfrom os.path import isfile, join  # For path manipulations and checking file existence\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List all the files in the specified directory\nos.listdir('../input/gan-getting-started/')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define directories containing images\nmonet_dir = '../input/gan-getting-started/monet_jpg/'\nphoto_dir = '../input/gan-getting-started/photo_jpg/'\n\n# Retrieve filenames for each directory\nmonet_filenames = [join(monet_dir, f) for f in listdir(monet_dir) if isfile(join(monet_dir, f))]\nphoto_filenames = [join(photo_dir, f) for f in listdir(photo_dir) if isfile(join(photo_dir, f))]\n\n# Load images and convert them to NumPy arrays\nmonet_files = [np.array(Image.open(f)) for f in monet_filenames]\nphoto_files = [np.array(Image.open(f)) for f in photo_filenames]\n\n# Print metrics about the loaded images\nprint(f\"Number of Monet files: {len(monet_files)}\")\nprint(f\"Number of Photo files: {len(photo_files)}\")\nprint(f\"Example image shape: {monet_files[1].shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA Procedure","metadata":{}},{"cell_type":"markdown","source":"Show the sample image from the 13th.","metadata":{}},{"cell_type":"code","source":"# Display examples of images\nplt.figure(figsize=(10, 5))\n\n# Display a sample photo\nplt.subplot(1, 2, 1)\nplt.title('Photo')\nplt.imshow(mpimg.imread(photo_filenames[13]))\nplt.axis('off')  # Hide axes for cleaner visualization\n\n# Display a sample Monet painting\nplt.subplot(1, 2, 2)\nplt.title('Monet')\nplt.imshow(mpimg.imread(monet_filenames[13]))\nplt.axis('off')  # Hide axes for cleaner visualization\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Show the sample image from the 15th.","metadata":{}},{"cell_type":"code","source":"# Display examples of images\nplt.figure(figsize=(10, 5))\n\n# Display a sample photo\nplt.subplot(1, 2, 1)\nplt.title('Photo')\nplt.imshow(mpimg.imread(photo_filenames[15]))\nplt.axis('off')  # Hide axes for cleaner visualization\n\n# Display a sample Monet painting\nplt.subplot(1, 2, 2)\nplt.title('Monet')\nplt.imshow(mpimg.imread(monet_filenames[15]))\nplt.axis('off')  # Hide axes for cleaner visualization\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's write a loop to preview data (as necessary.)","metadata":{}},{"cell_type":"code","source":"for i in [1,2,3]:\n    \n    # Display examples of images\n    plt.figure(figsize=(10, 5))\n\n    # Display a sample photo\n    plt.subplot(1, 2, 1)\n    plt.title('Photo')\n    plt.imshow(mpimg.imread(photo_filenames[i]))\n    plt.axis('off')  # Hide axes for cleaner visualization\n\n    # Display a sample Monet painting\n    plt.subplot(1, 2, 2)\n    plt.title('Monet')\n    plt.imshow(mpimg.imread(monet_filenames[i]))\n    plt.axis('off')  # Hide axes for cleaner visualization\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"class TorchDataset(Dataset):\n    def __init__(self, data_path, transform=None):\n        self.data_path = data_path\n        self.transform = transform\n        self.image_paths = [os.path.join(data_path, file) for file in os.listdir(data_path) if file.endswith('.jpg')]\n        \n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transforms = torchvision.transforms.Compose([\n    torchvision.transforms.Resize((256,256)),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MonetDataset = TorchDataset(data_path='/kaggle/input/gan-getting-started/monet_jpg', transform=transforms)\nPhotoDataset = TorchDataset(data_path='/kaggle/input/gan-getting-started/photo_jpg', transform=transforms)\n\nMonetLoader = DataLoader(MonetDataset, batch_size=16, shuffle=True, num_workers=2)\nPhotoLoader = DataLoader(PhotoDataset, batch_size=16, shuffle=True, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(MonetDataset)\nprint(PhotoDataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(MonetLoader)\nprint(PhotoLoader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the monet tensor\nprint(next(iter(MonetDataset)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect the photo tensor\nprint(next(iter(PhotoDataset)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monets = next(iter(MonetLoader))\nphotos = next(iter(PhotoLoader))\nprint(monets.shape, photos.shape)\n\n# tansform data to be readable by plt.imshow\nmonets_imshow = []\nfor monet in monets:\n    monet = monet.permute(1, 2, 0) \n    monet = monet.numpy() \n    monet = monet * 0.5 + 0.5 \n    monets_imshow.append(monet)\n\nphotos_imshow = []    \nfor photo in photos:\n    photo = photo.permute(1, 2, 0) \n    photo = photo.numpy()\n    photo = photo * 0.5 + 0.5\n    photos_imshow.append(photo)\n    \nprint(monets_imshow[3].shape) \nprint(monets_imshow[3].dtype) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Monet')\nplt.figure(figsize=(15, 5)) \nfor i in range(4):\n    plt.subplot(1, 4, i+1)  \n    plt.imshow(monets_imshow[i])\nplt.show()\n\nprint('Photos')\nplt.figure(figsize=(15, 5)) \nfor i in range(4):\n    plt.subplot(1, 4, i+1)  \n    plt.imshow(photos_imshow[i])\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building and Training\n\n### Build the Generator","metadata":{}},{"cell_type":"markdown","source":"![](https://hardikbansal.github.io/CycleGANBlog/images/model.jpg)","metadata":{}},{"cell_type":"markdown","source":"High level structure of Generator can be viewed in the following image.\n\n![](https://hardikbansal.github.io/CycleGANBlog/images/Generator.jpg)","metadata":{}},{"cell_type":"code","source":"class CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n        super().__init__()\n        \n        if down:\n            conv_layer = nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n        else:\n            conv_layer = nn.ConvTranspose2d(in_channels, out_channels, **kwargs)\n        \n        if use_act:\n            act_layer = nn.ReLU(inplace=True)\n        else:\n            act_layer = nn.Identity()\n        \n        self.cnn = nn.Sequential(\n            conv_layer,\n            nn.InstanceNorm2d(out_channels),\n            act_layer,\n        )\n\n    def forward(self, x):\n        return self.cnn(x)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            CNNBlock(channels, channels, kernel_size=3, padding=1),\n            CNNBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, img_channels, num_features=64, num_residuals=9):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(\n                img_channels,\n                num_features,\n                kernel_size=7,\n                stride=1,\n                padding=3,\n                padding_mode=\"reflect\",\n            ),\n            nn.InstanceNorm2d(num_features),\n            nn.ReLU(inplace=True),\n        )\n        self.down_blocks = nn.ModuleList(\n            [\n                CNNBlock(\n                    num_features, \n                    num_features * 2, \n                    kernel_size=3, \n                    stride=2, \n                    padding=1\n                ),\n                CNNBlock(\n                    num_features * 2,\n                    num_features * 4,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                ),\n            ]\n        )\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_features * 4) for _ in range(num_residuals)]\n        )\n        self.up_blocks = nn.ModuleList(\n            [\n                CNNBlock(\n                    num_features * 4,\n                    num_features * 2,\n                    down=False,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                ),\n                CNNBlock(\n                    num_features * 2,\n                    num_features * 1,\n                    down=False,\n                    kernel_size=3,\n                    stride=2,\n                    padding=1,\n                    output_padding=1,\n                ),\n            ]\n        )\n\n        self.last = nn.Conv2d(\n            num_features * 1,\n            img_channels,\n            kernel_size=7,\n            stride=1,\n            padding=3,\n            padding_mode=\"reflect\",\n        )\n\n    def forward(self, x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.res_blocks(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        return torch.tanh(self.last(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_channels = 3\nimg_size = 256\n\ntest_input = torch.randn((2, img_channels, img_size, img_size))\n\nmodel_gen_test = Generator(img_channels, 9)\n\nprint(model_gen_test(test_input).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Build the Discriminator","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            # Convolution layer\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=4,\n                stride=stride,\n                padding=1,\n                bias=True,\n                padding_mode=\"reflect\",\n            ), \n            nn.InstanceNorm2d(out_channels), \n            nn.LeakyReLU(0.2, inplace=True), \n        )\n    \n    def forward(self, x):\n        return self.cnn(x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                features[0],\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                padding_mode=\"reflect\",\n            ),\n            nn.LeakyReLU(0.2, inplace=True),\n        )\n\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            layers.append(\n                Block(in_channels, feature, stride=1 if feature == features[-1] else 2)\n            )\n            in_channels = feature\n        layers.append(\n            nn.Conv2d(\n                in_channels,\n                1,\n                kernel_size=4,\n                stride=1,\n                padding=1,\n                padding_mode=\"reflect\",\n            )\n        )\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.initial(x)\n        return torch.sigmoid(self.model(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_dis_test = Discriminator()\n\ntest_input = torch.randn((2, 3, 256, 256))\n\nprint(model_dis_test(test_input).shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\n# Calculate sizes for train and validation splits\nval_size_monet = int(0.2 * len(MonetDataset))\ntrain_size_monet = len(MonetDataset) - val_size_monet\n\nval_size_photo = int(0.2 * len(PhotoDataset))\ntrain_size_photo = len(PhotoDataset) - val_size_photo\n\n# Split datasets into training and validation sets\ntrain_MonetDataset, val_MonetDataset = random_split(MonetDataset, [train_size_monet, val_size_monet])\ntrain_PhotoDataset, val_PhotoDataset = random_split(PhotoDataset, [train_size_photo, val_size_photo])\n\n# Create DataLoaders for the training and validation sets\ntrain_MonetLoader = DataLoader(train_MonetDataset, batch_size=16, shuffle=True, num_workers=2)\nval_MonetLoader = DataLoader(val_MonetDataset, batch_size=16, shuffle=False, num_workers=2)\n\ntrain_PhotoLoader = DataLoader(train_PhotoDataset, batch_size=16, shuffle=True, num_workers=2)\nval_PhotoLoader = DataLoader(val_PhotoDataset, batch_size=16, shuffle=False, num_workers=2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm import tqdm\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nG_Photo2Monet = Generator(img_channels=3).to(device)\nG_Monet2Photo = Generator(img_channels=3).to(device)\nD_Photo = Discriminator(in_channels=3).to(device)\nD_Monet = Discriminator(in_channels=3).to(device)\n\noptimizer_G_Photo2Monet = optim.Adam(G_Photo2Monet.parameters(), lr=1e-4, betas=(0.5, 0.5))\noptimizer_G_Monet2Photo = optim.Adam(G_Monet2Photo.parameters(), lr=1e-4, betas=(0.5, 0.5))\noptimizer_D_Photo = optim.Adam(D_Photo.parameters(), lr=1e-4, betas=(0.5, 0.5))\noptimizer_D_Monet = optim.Adam(D_Monet.parameters(), lr=1e-4, betas=(0.5, 0.5))\n\ncriterion_GAN = nn.MSELoss()\ncriterion_cycle = nn.L1Loss()\n\nscaler = GradScaler()\n\nlambda_cycle = 5\nnum_epochs = 100\nbatch_size = 16\n\nfor epoch in range(num_epochs):\n    epoch_loss_G_Photo2Monet = 0\n    epoch_loss_G_Monet2Photo = 0\n    epoch_loss_D_Photo = 0\n    epoch_loss_D_Monet = 0\n    \n    for i, (real_photo, real_monet) in enumerate(zip(train_PhotoLoader, train_MonetLoader)):\n        real_photo = real_photo.to(device)\n        real_monet = real_monet.to(device)\n        batch_size = real_photo.size(0)\n\n        fake_monet = G_Photo2Monet(real_photo).detach()\n        fake_photo = G_Monet2Photo(real_monet).detach()\n\n        D_Photo_real = D_Photo(real_photo)\n        D_Monet_real = D_Monet(real_monet)\n        D_Photo_output_shape = D_Photo_real.shape\n        D_Monet_output_shape = D_Monet_real.shape\n\n        real_label_Photo = torch.ones(D_Photo_output_shape, dtype=torch.float, device=device)\n        fake_label_Photo = torch.zeros(D_Photo_output_shape, dtype=torch.float, device=device)\n        real_label_Monet = torch.ones(D_Monet_output_shape, dtype=torch.float, device=device)\n        fake_label_Monet = torch.zeros(D_Monet_output_shape, dtype=torch.float, device=device)\n\n        with autocast():\n            D_Photo_real = D_Photo(real_photo)\n            D_Photo_fake = D_Photo(fake_photo)\n            loss_D_Photo_real = criterion_GAN(D_Photo_real, real_label_Photo)\n            loss_D_Photo_fake = criterion_GAN(D_Photo_fake, fake_label_Photo)\n            loss_D_Photo = (loss_D_Photo_real + loss_D_Photo_fake) * 0.5\n\n            D_Monet_real = D_Monet(real_monet)\n            D_Monet_fake = D_Monet(fake_monet)\n            loss_D_Monet_real = criterion_GAN(D_Monet_real, real_label_Monet)\n            loss_D_Monet_fake = criterion_GAN(D_Monet_fake, fake_label_Monet)\n            loss_D_Monet = (loss_D_Monet_real + loss_D_Monet_fake) * 0.5\n\n        optimizer_D_Photo.zero_grad()\n        scaler.scale(loss_D_Photo).backward()\n        scaler.step(optimizer_D_Photo)\n\n        optimizer_D_Monet.zero_grad()\n        scaler.scale(loss_D_Monet).backward()\n        scaler.step(optimizer_D_Monet)\n\n        epoch_loss_D_Photo += loss_D_Photo.item()\n        epoch_loss_D_Monet += loss_D_Monet.item()\n\n        with autocast():\n            fake_monet = G_Photo2Monet(real_photo)\n            fake_photo = G_Monet2Photo(real_monet)\n            loss_GAN_Photo2Monet = criterion_GAN(D_Monet(fake_monet), real_label_Monet)\n            loss_GAN_Monet2Photo = criterion_GAN(D_Photo(fake_photo), real_label_Photo)\n\n            cycle_photo = G_Monet2Photo(fake_monet)\n            cycle_monet = G_Photo2Monet(fake_photo)\n            loss_cycle_photo = criterion_cycle(cycle_photo, real_photo)\n            loss_cycle_monet = criterion_cycle(cycle_monet, real_monet)\n\n            loss_G_Photo2Monet = loss_GAN_Photo2Monet + lambda_cycle * loss_cycle_photo\n            loss_G_Monet2Photo = loss_GAN_Monet2Photo + lambda_cycle * loss_cycle_monet\n\n        optimizer_G_Photo2Monet.zero_grad()\n        scaler.scale(loss_G_Photo2Monet).backward()\n        scaler.step(optimizer_G_Photo2Monet)\n\n        optimizer_G_Monet2Photo.zero_grad()\n        scaler.scale(loss_G_Monet2Photo).backward()\n        scaler.step(optimizer_G_Monet2Photo)\n\n        scaler.update()\n\n        epoch_loss_G_Photo2Monet += loss_G_Photo2Monet.item()\n        epoch_loss_G_Monet2Photo += loss_G_Monet2Photo.item()\n\n    epoch_loss_G_Photo2Monet /= len(train_PhotoLoader)\n    epoch_loss_G_Monet2Photo /= len(train_MonetLoader)\n    epoch_loss_D_Photo /= len(train_PhotoLoader)\n    epoch_loss_D_Monet /= len(train_MonetLoader)\n\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n          f\"loss_G_Photo2Monet: {epoch_loss_G_Photo2Monet:.4f}, \"\n          f\"loss_G_Monet2Photo: {epoch_loss_G_Monet2Photo:.4f}, \"\n          f\"loss_D_Photo: {epoch_loss_D_Photo:.4f}, \"\n          f\"loss_D_Monet: {epoch_loss_D_Monet:.4f}\")\n\n    if (epoch + 1) % 10 == 0:\n        with torch.no_grad():\n            G_Photo2Monet.eval()\n            example_photo = next(iter(val_PhotoLoader)).to(device)\n            fake_monet = G_Photo2Monet(example_photo)\n\n            plt.figure(figsize=(12, 6))\n            for i in range(4):\n                plt.subplot(2, 4, i + 1)\n                plt.imshow(((example_photo[i].cpu().numpy().transpose(1, 2, 0) + 1) / 2))\n                plt.axis('off')\n                plt.subplot(2, 4, i + 5)\n                plt.imshow(((fake_monet[i].cpu().numpy().transpose(1, 2, 0) + 1) / 2))\n                plt.axis('off')\n            plt.show()\n\n            G_Photo2Monet.train()\n\nfinal_model_path = 'final_model.pth'\ntorch.save({\n    'G_Photo2Monet': G_Photo2Monet.state_dict(),\n    'G_Monet2Photo': G_Monet2Photo.state_dict(),\n    'D_Photo': D_Photo.state_dict(),\n    'D_Monet': D_Monet.state_dict(),\n    'optimizer_G_Photo2Monet': optimizer_G_Photo2Monet.state_dict(),\n    'optimizer_G_Monet2Photo': optimizer_G_Monet2Photo.state_dict(),\n    'optimizer_D_Photo': optimizer_D_Photo.state_dict(),\n    'optimizer_D_Monet': optimizer_D_Monet.state_dict(),\n}, final_model_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nfrom zipfile import ZipFile\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_path = '/kaggle/working/final_model.pth'\ncheckpoint = torch.load(model_path)\n\n\nG_Photo2Monet = Generator(img_channels=3).to(device)\nG_Photo2Monet.load_state_dict(checkpoint['G_Photo2Monet'])\nG_Photo2Monet.eval()\n\ntest_loader = DataLoader(PhotoDataset, batch_size=1, shuffle=False)\n\n\nos.makedirs('../tmp/images', exist_ok=True)\n\nwith torch.no_grad():\n    for i, img in enumerate(test_loader):\n        img = img.to(device)\n        fake_monet = G_Photo2Monet(img)[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n        im = Image.fromarray((fake_monet * 255).astype(np.uint8))\n        im.save(f\"../tmp/images/{i}.jpg\")\n\noutput_zip_path = \"/kaggle/working/images.zip\"\nwith ZipFile(output_zip_path, 'w') as zipf:\n    for root, _, files in os.walk('../tmp/images'):\n        for file in files:\n            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), '../tmp/images'))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Additional Resources\n\n### Concepts of GANs\n- [Generative Adversarial Networks(GANs): Complete Guide to GANs](https://www.analyticsvidhya.com/blog/2021/10/an-end-to-end-introduction-to-generative-adversarial-networksgans/)\n- A CycleGAN operates through a structured process involving two core components: a generator and a discriminator. Both of these elements are built upon the foundation of conventional convolutional deep learning models. They utilize a series of layers designed to progressively reduce or augment the dimensions of an image. This intricate architecture allows the CycleGAN to effectively learn and transform images from one domain to another, leveraging the strengths of convolutional networks to manipulate image sizes through its generator and discriminator stages: [CycleGAN  |  TensorFlow Core](https://www.tensorflow.org/tutorials/generative/cyclegan)\n- [Mohammad-Rahmdel/CycleGAN: Tensorflow 2 implementation of CycleGAN(Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks)](https://github.com/Mohammad-Rahmdel/CycleGAN)\n- [LynnHo/CycleGAN-Tensorflow-2](https://github.com/LynnHo/CycleGAN-Tensorflow-2)\n- CycleGAN | Tensorflow Core. TensorFlow. (n.d.). Retrieved October 7, 2022, from https://www.tensorflow.org/tutorials/generative/cyclegan\n- Isola, P., Zhu, J.-Y., Zhou, T., & Efros, A. A. (2016). Image-to-image translation with conditional adversarial networks. Retrieved October 6, 2022, from Ανακτήθηκε από http://arxiv.org/abs/1611.07004\n- Jang, A. (2020, August 29). Monet Cyclegan Tutorial. Kaggle. Retrieved October 7, 2022, from https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook\n- nkmk. (n.d.). Convert BGR and RGB with python, opencv (cvtcolor). Convert BGR and RGB with Python, OpenCV (cvtColor). Retrieved October 7, 2022, from https://note.nkmk.me/en/python-opencv-bgr-rgb-cvtcolor/\n- Stack Overflow. (2014, March 3). Python - calculate histogram of image. Stack Overflow. Retrieved October 6, 2022, from https://stackoverflow.com/questions/22159160/python-calculate-histogram-of-image\n- Ulyanov, D., Vedaldi, A., & Lempitsky, V. (2016). Instance normalization: The missing ingredient for fast stylization. Retrieved October 6, 2022, from Ανακτήθηκε από http://arxiv.org/abs/1607.08022\n\n\n### TensorFlow & Keras\n- [TFRecord and tf.train.Example  |  TensorFlow Core](https://www.tensorflow.org/tutorials/load_data/tfrecord)\n\n### PyTorch\n\n- [Pytorch vs. TensorFlow: Which Framework to Choose? | by AnalytixLabs | Medium](https://medium.com/@byanalytixlabs/pytorch-vs-tensorflow-which-framework-to-choose-ed649d9e7a35)\n\n### Python Coding\n- [python - Limiting print output - Stack Overflow](https://stackoverflow.com/questions/29321969/limiting-print-output)\n- [python - How to read (decode) .tfrecords file, see the images inside and do augmentation? - Stack Overflow](https://stackoverflow.com/questions/65007191/how-to-read-decode-tfrecords-file-see-the-images-inside-and-do-augmentation)\n- [tensorflow/addons: Useful extra functionality for TensorFlow 2.x maintained by SIG-addons](https://github.com/tensorflow/addons#python-op-compatibility-matrix)\n- [python - How to find which version of TensorFlow is installed in my system? - Stack Overflow](https://stackoverflow.com/questions/38549253/how-to-find-which-version-of-tensorflow-is-installed-in-my-system)\n- [python - ImportError: No module named keras.optimizers - Stack Overflow](https://stackoverflow.com/questions/39081910/importerror-no-module-named-keras-optimizers)","metadata":{}}]}